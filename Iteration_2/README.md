
# ğŸ§  TinyLLaMA-Powered CV & Chatbot APIs

This repository provides two AI-driven microservices built with **FastAPI** and **llama.cpp**, using local LLaMA-compatible models (GGUF format). These tools are optimized for deployment in resource-efficient environments (such as virtual machines on Azure) without depending on third-party APIs or internet connectivity.

---

## ğŸ“Œ Project Purpose

This project addresses the needs of mature-age job seekers (e.g., aged 50â€“65) by offering two intelligent features:

1. **CV Generator API** â€“ Automatically builds professional resume text from structured data, enabling non-technical users to create clean and confident self-presentations.
2. **Chatbot API** â€“ Offers personalized, context-aware guidance on job-seeking, re-skilling, and addressing common concerns (e.g., employment gaps).

Both services prioritize data privacy, local control, and transparent AI behavior by operating fully offline with TinyLLaMA.

---

## ğŸ§± Architecture Overview

```
[User Input]
     â†“
[FastAPI Endpoint]
     â†“
[Prompt Builder] -- [System Prompt (Chatbot)] or [Sectional Prompts (CV)]
     â†“
[TinyLLaMA Inference via llama.cpp]
     â†“
[Post-processor: Clean-up, SSE Streaming (if enabled)]
     â†“
[Response Output or CV Text File]
```

Each module (CV and Chatbot) is designed to be independently deployable, yet they share a similar pattern of prompt â†’ inference â†’ response.

---

## ğŸ“ Full Directory Layout

```
tinyllama1b/
â”œâ”€â”€ models/                         # Place GGUF models here
â”‚   â””â”€â”€ tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
â”‚
â”œâ”€â”€ python_proj/
â”‚   â”œâ”€â”€ chatbot/                    # Chatbot module
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app for /chatbot
â”‚   â”‚   â”œâ”€â”€ chat_session.py         # Maintains conversational context
â”‚   â”‚   â”œâ”€â”€ tiny_runner.py          # Low-level llama.cpp integration
â”‚   â”‚   â”œâ”€â”€ system_prompt.txt       # Instruction prompt (role: career advisor)
â”‚   â”‚   â”œâ”€â”€ cmds.txt                # Optional command list
â”‚   â”‚   â””â”€â”€ test_stream.html        # Frontend test tool (SSE enabled)
â”‚   â”‚
â”‚   â”œâ”€â”€ cv_builder/                 # CV generator module
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app for /generate_cv and /stream
â”‚   â”‚   â”œâ”€â”€ generator.py            # Core logic using llama inference
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py       # Builds section-based prompts
â”‚   â”‚   â”œâ”€â”€ cv_output.txt           # Logs final generated CV
â”‚   â”‚   â””â”€â”€ prompts/
â”‚   â”‚       â”œâ”€â”€ profile_prompt.txt
â”‚   â”‚       â”œâ”€â”€ edu_prompt.txt
â”‚   â”‚       â””â”€â”€ work_prompt.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ cmdlog.sh                   # CLI shortcut for testing
â”‚   â””â”€â”€ __pycache__/
â”‚
â”œâ”€â”€ server_setup.sh                 # Bash script for Azure deployment
â””â”€â”€ testing.sh                      # Batch tester or launcher
```

---

## âœï¸ CV Builder API

### What It Does
- Accepts user input (name, education, work history)
- Builds modular prompts for each section
- Calls TinyLLaMA to generate text responses for each part
- Returns structured JSON or streams each part via SSE
- Saves final result to `cv_output.txt`

### Key Files
- `cv_builder/main.py` â€“ FastAPI app routes
- `prompt_builder.py` â€“ Constructs full prompt dynamically
- `generator.py` â€“ Calls llama model and handles response

### How to Run
```bash
cd tinyllama1b/python_proj/cv_builder
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### Endpoints
- `POST /generate_cv` â€“ Full output in JSON
- `POST /generate_stream` â€“ Line-by-line streaming output

---

## ğŸ’¬ Chatbot API

### What It Does
- Accepts a question about job seeking (career planning, interview prep, etc.)
- Combines it with `system_prompt.txt` to stay on-topic
- Stores short session history in memory
- Responds using TinyLLaMA (Tiny Runner wrapper)

### Key Files
- `chatbot/main.py` â€“ FastAPI app
- `chat_session.py` â€“ Keeps prior exchanges
- `system_prompt.txt` â€“ Defines tone, role, context

### How to Run
```bash
cd tinyllama1b/python_proj/chatbot
uvicorn main:app --host 0.0.0.0 --port 8001 --reload
```

### Endpoint
- `POST /chatbot` â€“ Input: `{ "question": "..." }` â†’ Output: `{ "response": "..." }`

---

## ğŸ§  Model Integration & Optimization

- Model is loaded once per service on startup
- Uses `llama-cpp-python` backend for TinyLLaMA (GGUF format)
- Prompt + context is dynamically sized to stay within token limits
- No GPU dependency (runs well on CPU with quantized model)

---

## ğŸ§ª Testing & Debugging

### For CV Builder
```bash
curl -X POST http://localhost:8000/generate_cv -H "Content-Type: application/json" -d @test_cv.json
```

### For Chatbot
```bash
curl -X POST http://localhost:8001/chatbot -H "Content-Type: application/json" -d '{"question": "What skills should I learn to return to work?"}'
```

### HTML Debug Page
- `chatbot/test_stream.html` â€“ Open in browser to test SSE streaming

---

## ğŸ“¦ Installation

```bash
cd tinyllama1b/python_proj
pip install -r requirements.txt
```

Place your `.gguf` model under `tinyllama1b/models/` and update paths in `generator.py` or `tiny_runner.py` as needed.

---

## ğŸ” Privacy & Deployment Notes

- Runs entirely offline â€” no API calls are made to external services
- Best hosted on secure VM (e.g., Azure Ubuntu server with port 8000/8001)
- Logging only saved locally for debugging (no persistent storage)

---

## ğŸ“œ License

MIT License Â© 2025
